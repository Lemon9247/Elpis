# Elpis Default Configuration
# This file contains default settings for the Elpis agent.
# Copy to config.local.toml and customize as needed.

[model]
# Path to the GGUF model file
path = "./data/models/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf"

# Context window size in tokens
# For 32GB RAM systems: 32768 uses ~17GB for context (plus ~7GB for model)
# For 16GB RAM systems: 8192-16384 recommended
context_length = 32768

# Number of layers to offload to GPU (0 = CPU only, 35 = all layers)
gpu_layers = 35

# CPU threads to use for inference
n_threads = 8

# Temperature for sampling (higher = more creative)
temperature = 0.7

# Top-p sampling (nucleus sampling)
top_p = 0.9

# Maximum tokens to generate per response
max_tokens = 4096

# Hardware backend: "auto" (detect), "cuda" (NVIDIA), "rocm" (AMD), "cpu"
hardware_backend = "auto"

[tools]
# Workspace directory for agent file operations
workspace_dir = "./workspace"

# Maximum timeout for bash commands (seconds)
max_bash_timeout = 30

# Maximum file size for read operations (bytes)
max_file_size = 10485760  # 10MB

# Enable dangerous commands (rm -rf, etc.) - USE WITH CAUTION
enable_dangerous_commands = false

[logging]
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
level = "INFO"

# Log file path
output_file = "./logs/elpis.log"

# Log format: "json" or "text"
format = "json"
